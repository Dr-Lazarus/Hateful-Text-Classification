{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Stochastic GPA Descent Report\n",
    "\n",
    "- 1004318 Bryan Tan (Chen Zhengyu)\n",
    "- 1005254 Visshal Natarajan\n",
    "- 1004865 Wang Yanbao\n",
    "- 1004875 Xiang Siqi\n",
    "- 1005330 Christy Lau Jin Yun\n",
    "\n",
    "# Summary\n",
    "## Models Considered\n",
    "- **Logistic Regression**\n",
    "- **MultinomialNB**\n",
    "- **XGBoost** \n",
    "- **Extra Trees Classifier**\n",
    "- **VotingClassifier**\n",
    "\n",
    "## Other libraries used\n",
    "- **TPOT**: Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. Can output pipelines that combines/stack multiples models + preprocessors in its default state.\n",
    "- **Optuna**: Hyperparameter tuning framework which can find the optimal machine learning in a large search space comparable to that of Random Grid Search, but with smarter choices of hyperparameters in each iteration and faster rate of model evaluation, ultimately leading to faster convergence.\n",
    "\n",
    "## Approaches to model training/evaluation\n",
    "- Evaluate all the above models using Stratified 3 Fold Cross Validation; most of them are optimised using Optuna\n",
    "- Utilise TPOT to search for machine learning pipelines that was not previously considered.\n",
    "\n",
    "## How we arrived at final model\n",
    "- ExtraTreesClassifier performed well on public dataset.\n",
    "- Pipelines output from TPOT also performed well on public dataset.\n",
    "- In order to prevent overfitting and improve model performance through the use of independent classifiers, we created a Voting Classifier with soft voting from a few classifier pipelines from TPOT (which we called `cl0`, `cl2` and `cl12`) with an Extra Trees Classifier (which we called `etc`).\n",
    "- We then used Optuna to optimise the weights assigned to each of the above classifiers.\n",
    "\n",
    "# Models Considered (Details)\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "### What is the model?\n",
    "* **Logistic Regression** is a linear model that outputs a probability (between 0 and 1) using the logistic function.\n",
    "  \n",
    "### Why we chose to test this model?\n",
    "* We use this model to provide a baseline f1_score which subsequent models ought to improve on.\n",
    "\n",
    "### Main Hyperparameters of the model\n",
    "* **penalty** {‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’\n",
    "<br> Specifies the norm of the penalty\n",
    "\n",
    "* **tol:** float, default = 1e-4\n",
    "<br> Tolerance for stopping criteria.\n",
    "\n",
    "* **C:** float, default = 1.0 <br> Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "  \n",
    "### Parameters tuning\n",
    "* None; we used the default value provided by Sci-kit learn\n",
    "\n",
    "### Result\n",
    "F1 score of Model on train-test split (75-25%): **0.6752**.\n",
    "\n",
    "## Multinomial NB\n",
    "### What is the model?\n",
    "* **Multinomial NB** implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes.\n",
    " ### Why we chose to test this model?\n",
    " * The reason behind testing this model is based on the research paper released regarding the classification of hate speech 'A comparison of classification algorithms for hate speech detection'. The results show that the Multinomial Naive Bayes algorithm produces the best model with the highest recall value of **93.2%** which has an accuracy value of **71.2%** for the classification of hate speech. (**Putri et al., 2020**).\n",
    "\n",
    "### Hyperparameters of the model\n",
    "* **alpha:** float, default = 1.0\n",
    "<br> Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "\n",
    "* **fit_prior:** bool, default = True\n",
    "<br> Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n",
    "\n",
    "* **class_prior:** array-like of shape (n_classes,), default = None <br> Prior probabilities of the classes. If specified, the priors are not adjusted according to the data\n",
    "\n",
    "### Parameters tuning\n",
    "* We utilized the **Optuna** hyperparameter optimization framework to tune our hyperparameters.\n",
    "\n",
    "### Why we chose this framework\n",
    "  *  Optuna boasts the following features:\n",
    "      - **Lightweight, versatile, and platform agnostic architecture**\n",
    "           - Handle a wide variety of tasks with a simple installation that has few requirements.\n",
    "       - **Pythonic search spaces**\n",
    "            - Define search spaces using familiar Python syntax including conditionals and loops.\n",
    "       - **Efficient optimization algorithms**\n",
    "            - Adopt state-of-the-art algorithms for sampling hyperparameters and efficiently pruning unpromising trials.\n",
    "       - **Easy parallelization**\n",
    "            - Scale studies to tens or hundreds or workers with little or no changes to the code.\n",
    "       - **Quick visualization**\n",
    "            - Inspect optimization histories from a variety of plotting functions.\n",
    "\n",
    "  *   Optuna also allowed us to find the optimal parameters at a faster rate as opposed to grid search though there is a trade off in accuracy.\n",
    "\n",
    "### Result\n",
    "F1 score of Model on public dataset (20%): **0.7100**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## XGBoost\n",
    "### What is the model?\n",
    "XGBoost is a **tree-based ensemble model** which stands for “gradient-boosted decision tree (GBDT)”. Compared to the random forest, XGBoost implement the idea of “boosting” which establishes a connection between trees. Therefore, trees in XGBoost models are no longer independent of each other and the model eventually becomes an orderly collective decision-making system.\n",
    "### Why we chose to test this model?\n",
    "XGBoost is an ensemble algorithm that has higher predicting power and performance, and it is achieved by improvisation on Gradient Boosting framework by introducing some accurate approximation algorithms. We speculate that by **gradient boost mechanism** and **self-regularization** embedded in the model can potentially increase the accuracy of predictions and reduce the risk of overfitting for tree-based models.\n",
    "\n",
    "### Hyperparameters of the model\n",
    "* **eta:** float, default = 0.3\n",
    "<br> The step size in fitting.\n",
    "\n",
    "* **objective:** default = reg:squarederror\n",
    "<br> Specify the learning task.\n",
    "\n",
    "* **num_round:** integer\n",
    "<br> Same as “n_estimators”, the number for boosting, which also decides the number of trees in the ensemble forest.\n",
    "\n",
    "* **subsample:** float, default = 1\n",
    "<br> Subsample ratio of the training instances (prevent overfitting).\n",
    "\n",
    "* **min_child_weight:** float, default = 1\n",
    "<br> Minimum sum of instance weight needed in a child. The larger, the more conservative the model will be.\n",
    "\n",
    "* **max_depth:** integer, default = 6\n",
    "<br> Maximum depth of a tree.\n",
    "\n",
    "* **gamma:** float, default = 0\n",
    "<br> Minimum loss reduction required to make a further partition on a leaf node.\n",
    "\n",
    "* **colsample_bytree:** default = 1\n",
    "<br> The subsample ratio of columns when constructing each tree.\n",
    "\n",
    "### Parameters tuning\n",
    "We are utilizing the **RandomSearchCV** provided by scikit-learn to tune our parameters.\n",
    "\n",
    "\n",
    "### Why we chose this framework\n",
    "* RandomSearchCV is useful when we have **many parameters** to try and the training time is very long. The **training time** for XGBoost is relatively long compared to non-tree-based models. Considering that cross-validation takes a longer time as well, the train load is even heavier.\n",
    "* The **number of parameters** to consider for XGBoost trees is particularly high and the magnitudes of influence are imbalanced. Compared to GridSearch, RandomSearch is more suitable in this situation.\n",
    "\n",
    "### Result\n",
    "F1 score of Model on public dataset (20%): **0.6607**\n",
    "\n",
    "### Learning points\n",
    "* **Overfitting problem:** It was shown during the training process that the f1-score on the training dataset was much higher than on the testing dataset under cross-validation (over 15% higher throughout the training process). Sometimes it happened that test loss didn't change while training loss was decreasing. Most **tree-based** models are easily overfitted. This means that increasing model complexity does not always increase model performance/robustness; **it's always about finding the balance between simplicity and complexity**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a762556",
   "metadata": {},
   "source": [
    "## Extra Tree Classifier (Included in the final Voting Classifier)\n",
    "\n",
    "### What is the model?\n",
    "It is an ensemble machine learning model that is derived from deciesion trees. Also known as \"Extremely Randomized Trees\".\n",
    "### Why we chose to test this model?\n",
    "ExtraTreesClassifier builds multiple trees and fits a number of randomized decision trees on various sub-samples of the dataset, averaging them to improve predictive accuracy and control over-fitting.\n",
    "\n",
    " \n",
    "####  Hyperparameters:\n",
    "* **n_estimators:** int, default = 100\n",
    "<br> The number of trees in the forest.\n",
    "\n",
    "* **criterion:** {“gini”, “entropy”, “log_loss”}, default = \"gini\"\n",
    "<br> The function to measure the quality of a split. \n",
    "\n",
    "* **max_depth:** int, default = None <br> The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "* **min_samples_split:** int or float, default = 2 <br> The minimum number of samples required to split an internal node. If int, then consider min_samples_split as the minimum number. If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "\n",
    "* **min_samples_leaf:** int or float, default = 1 <br> The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. \n",
    "  \n",
    "* **min_weight_fraction_leaf:** float, default = 0.0 <br> The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "\n",
    "* **max_features:** {“sqrt”, “log2”, None}, int or float, default = ”sqrt” <br> e number of features to consider when looking for the best split.If int, then consider max_features features at each split.\n",
    "\n",
    "If float, then max_features is a fraction and round(max_features * n_features) features are considered at each split. If “auto”, then max_features=sqrt(n_features). If “sqrt”, then max_features=sqrt(n_features). If “log2”, then max_features=log2(n_features). If None, then max_features=n_features.\n",
    "\n",
    "* **max_leaf_nodes:** int, default = None <br> Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
    "\n",
    "* **min_impurity_decrease:** float, default = 0.0 <br> A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "\n",
    "* **bootstrap:** bool, default=False <br> Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n",
    "\n",
    "* **random_state:** int, RandomState instance or None, default=None <br> Seed to control sources of randomness\n",
    "\n",
    "* **class_weight:** {“balanced”, “balanced_subsample”}, dict or list of dicts, default=None <br> Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. \n",
    "\n",
    "* **ccp_alpha:** non-negative float, default=0.0 <br> Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. \n",
    "\n",
    "* **max_samples:** int or float, default=None <br> If bootstrap is True, the number of samples to draw from X to train each base estimator.\n",
    "\n",
    "### Parameters tuning\n",
    "* We utilized the **Optuna** hyperparameter optimization framework to tune our hyperparameters.\n",
    "\n",
    "### Parameters obtained\n",
    "\n",
    "```\n",
    "model = ExtraTreesClassifier(n_estimators=144,\n",
    "                            max_depth=589,\n",
    "                            criterion=\"entropy\",\n",
    "                            class_weight=\"balanced_subsample\",\n",
    "                            ccp_alpha=6.267622143679782e-05,\n",
    "                            min_samples_split=157,\n",
    "                            min_weight_fraction_leaf=4.8022857076483334e-05,\n",
    "                            min_impurity_decrease=1.5576259402879695e-05,\n",
    "                            max_features=0.00502175457189458,\n",
    "                            max_samples=0.8999810323985775,\n",
    "                            bootstrap=True)\n",
    "\n",
    "```\n",
    "\n",
    "### Results\n",
    "F1 score of Model on public dataset (20%) = **0.72273**\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c567f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **Out of the models evaluated above, Extra Trees Classifier performed the best. However, in addition to testing single models, we also attempted to use a library to generate pipelines that perform well with the given dataset. This was done using TPOT.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c670a844",
   "metadata": {},
   "source": [
    "## **TPOT**: An pipeline-generating AutoML library using genetic programming\n",
    "\n",
    "####  What is TPOT ?\n",
    "\n",
    "  * **TPOT** stands for Tree-based Pipeline Optimization Tool. It is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming\n",
    "\n",
    "    \n",
    " ####  Why we chose to use TPOT ? \n",
    "  \n",
    " * TPOT automates the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one the given data. \n",
    " * It is built on scikit-learn (familiar interface)\n",
    "\n",
    " ####  Key parameters of TPOT \n",
    "* **generations:** int or None, optional. default = 100\n",
    "<br> Number of iterations to the run pipeline optimization process.\n",
    "\n",
    "* **population_size:** nt, optional (default=100)\n",
    "<br> Number of individuals to retain in the genetic programming population every generation. Must be a positive number.\n",
    "\n",
    "* **scoring:** string or callable, optional (default='accuracy') <br> Function used to evaluate the quality of a given pipeline for the classification problem. The following built-in scoring functions can be used:\n",
    "\n",
    "* **cv:** int, cross-validation generator, or an iterable, optional (default=5) <br> Cross-validation strategy used when evaluating pipelines.\n",
    "\n",
    "* **config_dict:** Python dictionary, string, or None, optional (default=None) <br> A configuration dictionary for customizing the operators and parameters that TPOT searches in the optimization process.\n",
    "  \n",
    "\n",
    "* **random_state:** int, RandomState instance or None, default=None <br> he seed of the pseudo random number generator used in TPOT.\n",
    "\n",
    "\n",
    " #### Implementation:\n",
    "```\n",
    "import random\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "print(\"reading CSVs\")\n",
    "train_df = pd.read_csv(\"/kaggle/input/50007-dataset/train_tfidf_features.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/50007-dataset/test_tfidf_features.csv\")\n",
    "\n",
    "print(\"creating X and y\")\n",
    "X = train_df.drop(['id', 'label'], axis=1).values\n",
    "y = train_df['label'].values\n",
    "```\n",
    "<details>\n",
    "<summary>(click to expand) searchDict = </summary>\n",
    "{\n",
    "\n",
    "    # Classifiers\n",
    "    'sklearn.naive_bayes.BernoulliNB': {\n",
    "        'alpha': [1.882, 5, 0.1],\n",
    "        'fit_prior': [False]\n",
    "    },\n",
    "    \n",
    "    'sklearn.ensemble.ExtraTreesClassifier': {\n",
    "        'n_estimators': [100],\n",
    "        'criterion': [\"gini\"],\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        \"max_depth\": range(1, 2002,100),\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "\n",
    "    'sklearn.tree.DecisionTreeClassifier': {\n",
    "        'criterion': [\"gini\", \"entropy\"],\n",
    "        'max_depth': range(1, 11),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'ccp_alpha':[0.0001,0.001,0.005,0.01,0.05]\n",
    "    },\n",
    "    \n",
    "    'sklearn.svm.LinearSVC': {\n",
    "        'penalty': [\"l1\", \"l2\"],\n",
    "        'loss': [\"hinge\", \"squared_hinge\"],\n",
    "        'dual': [True, False],\n",
    "        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.]\n",
    "    },\n",
    "\n",
    "\n",
    "    'sklearn.linear_model.SGDClassifier': {\n",
    "        'loss': ['log', 'hinge', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        'penalty': ['elasticnet'],\n",
    "        'alpha': [3e-06, 0.01, 0.001],\n",
    "        'learning_rate': ['invscaling'],\n",
    "        'fit_intercept': [True, False],\n",
    "        'l1_ratio': [0.25, 0.0, 1.0, 0.75, 0.5],\n",
    "        'eta0': [0.1, 1.0, 0.01,0.0012],\n",
    "        'power_t': [0.5, 0.0, 1.0, 0.1, 100.0, 10.0, 50.0]\n",
    "    },\n",
    "\n",
    "    # Preprocesssors\n",
    "    'sklearn.preprocessing.Binarizer': {\n",
    "        'threshold': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.cluster.FeatureAgglomeration': {\n",
    "        'linkage': ['ward', 'complete', 'average'],\n",
    "        'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine']\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.MaxAbsScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.MinMaxScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.Normalizer': {\n",
    "        'norm': ['l1', 'l2', 'max']\n",
    "    },\n",
    "\n",
    "    'sklearn.kernel_approximation.RBFSampler': {\n",
    "        'gamma': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.RobustScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.StandardScaler': {\n",
    "    },\n",
    "\n",
    "    'tpot.builtins.ZeroCount': {\n",
    "    },\n",
    "\n",
    "    # Selectors\n",
    "    'sklearn.feature_selection.SelectFwe': {\n",
    "        'alpha': np.arange(0, 0.05, 0.001),\n",
    "        'score_func': {\n",
    "            'sklearn.feature_selection.f_classif': None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.SelectPercentile': {\n",
    "        'percentile': range(1, 100),\n",
    "        'score_func': {\n",
    "            'sklearn.feature_selection.f_classif': None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.VarianceThreshold': {\n",
    "        'threshold': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "</details>\n",
    "\n",
    "```\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=3)\n",
    "\n",
    "periodic_checkpoint_folder = \"/kaggle/working/checkpoints/\"\n",
    "model = TPOTClassifier(generations=500, population_size=30, cv=cv, config_dict=searchDict,\n",
    "                       scoring='f1_macro', verbosity=3, random_state=random.randint(1, 999999999),\n",
    "                       n_jobs=2, periodic_checkpoint_folder=periodic_checkpoint_folder,\n",
    "                       max_time_mins=700, max_eval_time_mins=15)\n",
    "# perform the search\n",
    "model.fit(X, y)\n",
    "# export the best model\n",
    "model.export('tpot_best_model.py')\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a707f999",
   "metadata": {},
   "source": [
    "#### **Running the above implmentation on several kaggle notebooks for 12 hours each yield various well-performing cross-validated pipelines. The three selected pipelines are as follows:**\n",
    "\n",
    "```\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import Normalizer, FunctionTransformer, RobustScaler, MaxAbsScaler, Binarizer, MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectPercentile, f_classif\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from tpot.builtins import StackingEstimator\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from copy import copy\n",
    "\n",
    "# Average CV score on the training set was: 0.7165325962375743\n",
    "cl0 = make_pipeline(\n",
    "    make_union(\n",
    "        FunctionTransformer(copy),\n",
    "        make_pipeline(\n",
    "            make_union(\n",
    "                Normalizer(norm=\"max\"),\n",
    "                SelectPercentile(score_func=f_classif, percentile=5)\n",
    "            ),\n",
    "            MaxAbsScaler()\n",
    "        )\n",
    "    ),\n",
    "    StackingEstimator(estimator=SGDClassifier(alpha=0.001, eta0=1.0, fit_intercept=False, l1_ratio=0.5,\n",
    "                                              learning_rate=\"invscaling\", loss=\"modified_huber\", penalty=\"elasticnet\", power_t=0.5)),\n",
    "    BernoulliNB(alpha=1.0, fit_prior=True)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(cl0.steps, 'random_state', 2)\n",
    "\n",
    "\n",
    "# Average CV score on the training set was: 0.7167734749214567\n",
    "cl2 = make_pipeline(\n",
    "    StackingEstimator(estimator=DecisionTreeClassifier(\n",
    "        criterion=\"gini\", max_depth=8, min_samples_leaf=9, min_samples_split=6)),\n",
    "    StackingEstimator(estimator=SGDClassifier(alpha=0.001, eta0=1.0, fit_intercept=False, l1_ratio=0.75,\n",
    "                                              learning_rate=\"invscaling\", loss=\"perceptron\", penalty=\"elasticnet\", power_t=0.1)),\n",
    "    BernoulliNB(alpha=1.0, fit_prior=False)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(cl2.steps, 'random_state', 222)\n",
    "\n",
    "\n",
    "# Average CV score on the training set was: 0.7192365888770997\n",
    "cl12 = make_pipeline(\n",
    "    SelectPercentile(score_func=f_classif, percentile=77),\n",
    "    SelectPercentile(score_func=f_classif, percentile=68),\n",
    "    StackingEstimator(estimator=SGDClassifier(alpha=0.001, eta0=0.0012, fit_intercept=False,\n",
    "                                              l1_ratio=0.0, learning_rate=\"invscaling\", loss=\"hinge\", penalty=\"elasticnet\", power_t=0.5)),\n",
    "    BernoulliNB(alpha=2.300000000000001, fit_prior=False)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(cl12.steps, 'random_state', 422)\n",
    "\n",
    "```\n",
    "#### Because there is an extremely large variety of preprocessors and hyperparameters defined above, **we will exclude the explaination of the preprocessors and only attempt to explain the main models used in the above 3 pipelines**\n",
    "\n",
    "### Which models are included?\n",
    "\n",
    "#### 1. **SGD Classifier:**\n",
    "##### Explanation:\n",
    "* implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification. Below is the decision boundary of a SGDClassifier trained with the hinge loss, equivalent to a linear SVM.\n",
    "\n",
    "\n",
    "#### Hyperparameters:\n",
    "* **penalty:** {‘l2’, ‘l1’, ‘elasticnet’}, default=’l2’ <br>\n",
    "The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard regularizer for linear SVM models. ‘l1’ and ‘elasticnet’ might bring sparsity to the model (feature selection) not achievable with ‘l2’.\n",
    "\n",
    "* **alpha:** float, default=0.0001 <br>\n",
    "Constant that multiplies the regularization term. The higher the value, the stronger the regularization. Also used to compute the learning rate when set to learning_rate is set to ‘optimal’. Values must be in the range [0.0, inf).\n",
    "\n",
    "* **l1_ratio:** float, default=0.15 <br>\n",
    "The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Only used if penalty is ‘elasticnet’. Values must be in the range [0.0, 1.0].\n",
    "\n",
    "* **fit_intercept:** bool, default=True <br>\n",
    "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.\n",
    "\n",
    "* **max_iter:** int, default=1000 <br>\n",
    "The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit method. Values must be in the range [1, inf).\n",
    "\n",
    "\n",
    "* **tol:** float, default=1e-3 <br>\n",
    "The stopping criterion. If it is not None, training will stop when (loss > best_loss - tol) for n_iter_no_change consecutive epochs. Convergence is checked against the training loss or the validation loss depending on the early_stopping parameter. Values must be in the range [0.0, inf).\n",
    "\n",
    "#### 2. **Decision Tree Classifier**:\n",
    "\n",
    "##### Explanation:\n",
    "* A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.\n",
    "\n",
    "####  Hyperparameters:\n",
    "* **criterion:** {“gini”, “entropy”, “log_loss”}, default=”gini” <br>\n",
    "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “log_loss” and “entropy” both for the Shannon information gain, see Mathematical formulation.\n",
    "\n",
    "* **splitter:** {“best”, “random”}, default=”best” <br>\n",
    "The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
    " \n",
    "* **max_depth:** int, default=None <br>\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "* **min_samples_split:** int or float, default=2 <br>\n",
    "The minimum number of samples required to split an internal node:<br>\n",
    "If int, then consider min_samples_split as the minimum number.\n",
    "<br>If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "\n",
    "\n",
    "* **min_samples_leaf:** int or float, default=1 <br>\n",
    "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.<br>If int, then consider min_samples_leaf as the minimum number.\n",
    "<br>If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "\n",
    "\n",
    "* **min_weight_fraction_leaf:** float default=0.0 <br>\n",
    "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "\n",
    "* **max_features:** int or float {“auto”, “sqrt”, “log2”}, default=None <br>\n",
    "The number of features to consider when looking for the best split:<br>\n",
    "If int, then consider max_features features at each split.\n",
    "<br>If float, then max_features is a fraction and max(1, int(max_features * n_features_in_)) features are considered at each split.\n",
    "<br>If “auto”, then max_features=sqrt(n_features).\n",
    "<br>If “sqrt”, then max_features=sqrt(n_features).\n",
    "<br>If “log2”, then max_features=log2(n_features).\n",
    "<br>If None, then max_features=n_features.\n",
    "\n",
    "\n",
    "#### 3. **Bernoulli Naive Bayes**\n",
    "##### Explanation:\n",
    "* Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. \n",
    "\n",
    "#### Hyperparameters:\n",
    "* **alpha:** float, default=1.0 <br>\n",
    "Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "\n",
    "* **binarize:** float or None, default=0.0 <br>\n",
    "Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors.\n",
    "\n",
    "* **fit_prior:** bool, default=True <br>\n",
    "Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n",
    "\n",
    "* **class_prior:** array-like of shape (n_classes,), default=None <br>\n",
    "Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e113c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# FINAL MODEL CREATION\n",
    "\n",
    "### In order to craft a model that is performant and resilient, we used a voting classifier to aggregate the predicted probabilties of the Extra Trees Classifier and 3 of the best performing TPOT pipelines into a single model.\n",
    "\n",
    "### The rationale is that a variety of model that is independent is able to resolve and adjust for the errors of any single model included within the ensemble\n",
    "\n",
    "## VotingClassifier\n",
    "\n",
    "### What is the model?\n",
    "* **VotingClassifier** is a machine learning model that trains an ensemble of user-defined models and predicted an output based on the weights (voting power) assigned to each of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e40b9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Selected Models for the voting classifier\n",
    "\n",
    "* **1. Stacking Classifier1: SGD Classifier and Bernoulli Naive Bayes**\n",
    "    - **Preprocessing**: \n",
    "        - Values are rescaled by the maximum of the absolute values\n",
    "        - Select highest scoring percentage (5 percentile) of features\n",
    "        - Scale each feature by its maximum absolute value\n",
    "\n",
    "    - **SGD Classifier**\n",
    "\n",
    "    - **Bernoulli Naive Bayes**\n",
    "    - **Individual Performance on cross validation**: `0.7165325962375743`\n",
    "\n",
    "### **Code for first model from TPOT**\n",
    "\n",
    "```\n",
    "# Average CV score on the training set was: 0.7165325962375743\n",
    "cl0 = make_pipeline(\n",
    "    make_union(\n",
    "        FunctionTransformer(copy),\n",
    "        make_pipeline(\n",
    "            make_union(\n",
    "                Normalizer(norm=\"max\"),\n",
    "                SelectPercentile(score_func=f_classif, percentile=5)\n",
    "            ),\n",
    "            MaxAbsScaler()\n",
    "        )\n",
    "    ),\n",
    "    StackingEstimator(estimator=SGDClassifier(alpha=0.001, eta0=1.0, fit_intercept=False, l1_ratio=0.5,\n",
    "                                              learning_rate=\"invscaling\", loss=\"modified_huber\", penalty=\"elasticnet\", power_t=0.5)),\n",
    "    BernoulliNB(alpha=1.0, fit_prior=True)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(cl0.steps, 'random_state', 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119d57de",
   "metadata": {},
   "source": [
    "* **2. Stacking Classifier2: Decision Tree, SGD Classifier and Bernoulli Naive Bayes**\n",
    "    - **Decision Tree**\n",
    "    - **SGD classifier**\n",
    "    - **Bernoulli Naive Bayes**\n",
    "    - **Individual Performance**: `0.7167734749214567`\n",
    "### **Code for second model from TPOT**\n",
    "\n",
    "```\n",
    "# Average CV score on the training set was: 0.7167734749214567\n",
    "cl2 = make_pipeline(\n",
    "    StackingEstimator(estimator=DecisionTreeClassifier(\n",
    "        criterion=\"gini\", max_depth=8, min_samples_leaf=9, min_samples_split=6)),\n",
    "    StackingEstimator(estimator=SGDClassifier(alpha=0.001, eta0=1.0, fit_intercept=False, l1_ratio=0.75,\n",
    "                                              learning_rate=\"invscaling\", loss=\"perceptron\", penalty=\"elasticnet\", power_t=0.1)),\n",
    "    BernoulliNB(alpha=1.0, fit_prior=False)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(cl2.steps, 'random_state', 222)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d86fc8",
   "metadata": {},
   "source": [
    "* **3. Stacking Classifier3: SGD Classifier and Bernoulli Naive Bayes**\n",
    "    - **Preprocessing**:\n",
    "        - Select highest scoring percentage (77 percentile) of features\n",
    "        - Select highest scoring percentage (68 percentile) of features\n",
    "    - **Decision Tree**\n",
    "    - **SGD classifier**\n",
    "    - **Bernoulli Naive Bayes**\n",
    "    - **Individual Performance**: `0.7192365888770997`\n",
    "### **Code for last model from TPOT**\n",
    "\n",
    "```\n",
    "# Average CV score on the training set was: 0.7192365888770997\n",
    "cl12 = make_pipeline(\n",
    "    SelectPercentile(score_func=f_classif, percentile=77),\n",
    "    SelectPercentile(score_func=f_classif, percentile=68),\n",
    "    StackingEstimator(estimator=SGDClassifier(alpha=0.001, eta0=0.0012, fit_intercept=False,\n",
    "                                              l1_ratio=0.0, learning_rate=\"invscaling\", loss=\"hinge\", penalty=\"elasticnet\", power_t=0.5)),\n",
    "    BernoulliNB(alpha=2.300000000000001, fit_prior=False)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(cl12.steps, 'random_state', 422)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce5b78",
   "metadata": {},
   "source": [
    "* **4. Extra Tree Classifier**\n",
    "    - **Individual Performance**: `0.72273`\n",
    "    - **code for model 4**:\n",
    "### **Code for Optuna-optimised ExtraTreesClassifier**\n",
    "\n",
    "```\n",
    "etc = ExtraTreesClassifier(n_estimators=144,\n",
    "                           max_depth=589,\n",
    "                           criterion='entropy',\n",
    "                           class_weight='balanced_subsample',\n",
    "                           ccp_alpha=6.267622143679782e-05,\n",
    "                           min_samples_split=157,\n",
    "                           min_weight_fraction_leaf=4.8022857076483334e-05,\n",
    "                           min_impurity_decrease=1.5576259402879695e-05,\n",
    "                           max_features=0.00502175457189458,\n",
    "                           max_samples=0.8999810323985775,\n",
    "                           bootstrap=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14241ae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Voting Classifier: Brief Explanation and Parameters\n",
    "- **ExtraTreesClassifier** and **a mix of dissimilar models/pipelines** (3 pipelined stacking classifiers)\n",
    "- The Reason for this approach is to ensure that ExtraTreesClassifier does not overfit too much to the public test set on kaggle.\n",
    "- **ExtraTreesClassifier** had the highest score on the public test set on kaggle. However, we feared that it was overfitting to the public dataset, hence our inclusion of other models with ostensibly lower performance on the public test set.\n",
    "-  The TPOT models hence has a regularising effect on the overall model (we assess that the reduction in variance is worth the potential increase in bias)\n",
    "- `voting = 'soft'`: Predicts the class label based on the argmax of the sums of the predicted probabilities\n",
    "- `weight`: Multiple the predicted probability of each model with the assigned weight\n",
    "\n",
    "```\n",
    "estimatorsLast = [(\"cl0\", cl0), (\"cl2\", cl2), (\"cl12\", cl12), (\"etc\", etc)]\n",
    "w1 = 1\n",
    "w2 = 2\n",
    "w3 = 3.5\n",
    "w4 = 10\n",
    "weights = [w1, w2, w3, w4]\n",
    "model = VotingClassifier(estimators=estimatorsLast,\n",
    "                         voting='soft',\n",
    "                         verbose=False,\n",
    "                         n_jobs=2, weights=weights)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c2623",
   "metadata": {},
   "source": [
    "### How were the above weights chosen? It was based on a mix of an initial parameter search by Optuna, followed by manual tuning.\n",
    "\n",
    "#### What is optuna?\n",
    "* Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API. Optuna enjoys high modularity, and the user of Optuna can dynamically construct the search spaces for the hyperparameters.\n",
    "\n",
    "####   Why we chose this framework? (RECAP)\n",
    "\n",
    "  *  Optuna boasts the following features: \n",
    "      - **Lightweight, versatile, and platform agnostic architecture**\n",
    "           - Handle a wide variety of tasks with a simple installation that has few requirements.\n",
    "       - **Pythonic search spaces**\n",
    "            - Define search spaces using familiar Python syntax including conditionals and loops.\n",
    "       - **Efficient optimization algorithms**\n",
    "            - Adopt state-of-the-art algorithms for sampling hyperparameters and efficiently pruning unpromising trials.\n",
    "       - **Easy parallelization**\n",
    "            - Scale studies to tens or hundreds or workers with little or no changes to the code.\n",
    "       - **Quick visualization**\n",
    "            - Inspect optimization histories from a variety of plotting functions.\n",
    "     \n",
    "     Optuna also allowed us to find the optimal parameters at a faster rate as opposed to grid search though there is a trade off in accuracy. \n",
    "     \n",
    "#### Implementation:\n",
    "\n",
    "```\n",
    "import optuna\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "    ### modeling with suggested params\n",
    "    w1 = trial.suggest_float(\"w1\", 0.5, 6)\n",
    "    w2 = trial.suggest_float(\"w2\", 0.5, 6)\n",
    "    w3 = trial.suggest_float(\"w3\", 0.5, 6)\n",
    "    w4 = trial.suggest_float(\"w4\", 0.5, 6)\n",
    "\n",
    "    weights = [w1, w2, w3, w4]\n",
    "\n",
    "    # Average CV score on the training set was: 0.7165325962375743\n",
    "    cl0 = make_pipeline(\n",
    "        make_union(\n",
    "            FunctionTransformer(copy),\n",
    "            make_pipeline(\n",
    "                make_union(\n",
    "                    Normalizer(norm=\"max\"),\n",
    "                    SelectPercentile(score_func=f_classif, percentile=5)\n",
    "                ),\n",
    "                MaxAbsScaler()\n",
    "            )\n",
    "        ),\n",
    "        StackingEstimator(estimator=SGDClassifier(alpha=0.001, eta0=1.0, fit_intercept=False, l1_ratio=0.5,\n",
    "                                                  learning_rate=\"invscaling\", loss=\"modified_huber\", penalty=\"elasticnet\", power_t=0.5)),\n",
    "        BernoulliNB(alpha=1.0, fit_prior=True)\n",
    "    )\n",
    "    # Fix random state for all the steps in exported pipeline\n",
    "    set_param_recursive(cl0.steps, 'random_state', 2)\n",
    "\n",
    "    # Average CV score on the training set was: 0.7167734749214567\n",
    "    cl2 = make_pipeline(\n",
    "        StackingEstimator(estimator=DecisionTreeClassifier(\n",
    "            criterion=\"gini\", max_depth=8, min_samples_leaf=9, min_samples_split=6)),\n",
    "        StackingEstimator(estimator=SGDClassifier(alpha=0.001, eta0=1.0, fit_intercept=False, l1_ratio=0.75,\n",
    "                                                  learning_rate=\"invscaling\", loss=\"perceptron\", penalty=\"elasticnet\", power_t=0.1)),\n",
    "        BernoulliNB(alpha=1.0, fit_prior=False)\n",
    "    )\n",
    "    # Fix random state for all the steps in exported pipeline\n",
    "    set_param_recursive(cl2.steps, 'random_state', 222)\n",
    "\n",
    "    # Average CV score on the training set was: 0.7192365888770997\n",
    "    cl12 = make_pipeline(\n",
    "        SelectPercentile(score_func=f_classif, percentile=77),\n",
    "        SelectPercentile(score_func=f_classif, percentile=68),\n",
    "        StackingEstimator(estimator=SGDClassifier(alpha=0.001, eta0=0.0012, fit_intercept=False,\n",
    "                                                  l1_ratio=0.0, learning_rate=\"invscaling\", loss=\"hinge\", penalty=\"elasticnet\", power_t=0.5)),\n",
    "        BernoulliNB(alpha=2.300000000000001, fit_prior=False)\n",
    "    )\n",
    "    # Fix random state for all the steps in exported pipeline\n",
    "    set_param_recursive(cl12.steps, 'random_state', 422)\n",
    "\n",
    "    etc = ExtraTreesClassifier(n_estimators=144,\n",
    "                               max_depth=589,\n",
    "                               criterion='entropy',\n",
    "                               class_weight='balanced_subsample',\n",
    "                               ccp_alpha=6.267622143679782e-05,\n",
    "                               min_samples_split=157,\n",
    "                               min_weight_fraction_leaf=4.8022857076483334e-05,\n",
    "                               min_impurity_decrease=1.5576259402879695e-05,\n",
    "                               max_features=0.00502175457189458,\n",
    "                               max_samples=0.8999810323985775,\n",
    "                               bootstrap=True)\n",
    "    estimatorsLast = [(\"cl0\", cl0), (\"cl2\", cl2), (\"cl12\", cl12), (\"etc\", etc)]\n",
    "    \n",
    "    model = VotingClassifier(estimators=estimatorsLast,\n",
    "                             voting='soft',\n",
    "                             verbose=False,\n",
    "                             n_jobs=2, weights=weights)\n",
    "    ## cross validation score\n",
    "    score = cross_val_score(model, X, y, n_jobs=2, cv=cv, scoring=\"f1_macro\")\n",
    "    f1_mean = score.mean()\n",
    "\n",
    "    return f1_mean\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')  # maximize accuracy\n",
    "study.optimize(objective, n_trials=None, timeout=42000, n_jobs=2,)\n",
    "print(study.best_trial.params)\n",
    "print(study.best_value)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dfdc51",
   "metadata": {},
   "source": [
    "#### Running the above Optuna search on a kaggle notebook for 12 hours resulted in the following weights:\n",
    "\n",
    "```\n",
    "{'w1': 0.5053634333272882,\n",
    " 'w2': 0.8790582376550015,\n",
    " 'w3': 1.4825640308089136,\n",
    " 'w4': 5.669220731735056}\n",
    "```\n",
    "\n",
    "#### This was manually tuned to arrive at the final chosen weight:\n",
    "\n",
    "```\n",
    "w1 = 1\n",
    "w2 = 2\n",
    "w3 = 3.5\n",
    "w4 = 10\n",
    "```\n",
    "\n",
    "# FINAL MODEL USED \n",
    "\n",
    "```\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import Normalizer, FunctionTransformer, RobustScaler, MaxAbsScaler, Binarizer, MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectPercentile, f_classif\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from tpot.builtins import StackingEstimator\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from copy import copy\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# Weights chosen after Optuna hyperparamter tuning and manual adjustments\n",
    "w1 = 1\n",
    "w2 = 2\n",
    "w3 = 3.5\n",
    "w4 = 10\n",
    "weights = [w1, w2, w3, w4]\n",
    "\n",
    "cl0 = make_pipeline(\n",
    "    make_union(\n",
    "        FunctionTransformer(copy),\n",
    "        make_pipeline(\n",
    "            make_union(\n",
    "                Normalizer(norm=\"max\"),\n",
    "                SelectPercentile(score_func=f_classif, percentile=5)\n",
    "            ),\n",
    "            MaxAbsScaler()\n",
    "        )\n",
    "    ),\n",
    "    StackingEstimator(estimator=SGDClassifier(alpha=0.001, eta0=1.0, fit_intercept=False, l1_ratio=0.5,\n",
    "                                              learning_rate=\"invscaling\", loss=\"modified_huber\", penalty=\"elasticnet\", power_t=0.5)),\n",
    "    BernoulliNB(alpha=1.0, fit_prior=True)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(cl0.steps, 'random_state', 2)\n",
    "\n",
    "# Average CV score on the training set was: 0.7167734749214567\n",
    "cl2 = make_pipeline(\n",
    "    StackingEstimator(estimator=DecisionTreeClassifier(\n",
    "        criterion=\"gini\", max_depth=8, min_samples_leaf=9, min_samples_split=6)),\n",
    "    StackingEstimator(estimator=SGDClassifier(alpha=0.001, eta0=1.0, fit_intercept=False, l1_ratio=0.75,\n",
    "                                              learning_rate=\"invscaling\", loss=\"perceptron\", penalty=\"elasticnet\", power_t=0.1)),\n",
    "    BernoulliNB(alpha=1.0, fit_prior=False)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(cl2.steps, 'random_state', 222)\n",
    "\n",
    "# Average CV score on the training set was: 0.7192365888770997\n",
    "cl12 = make_pipeline(\n",
    "    SelectPercentile(score_func=f_classif, percentile=77),\n",
    "    SelectPercentile(score_func=f_classif, percentile=68),\n",
    "    StackingEstimator(estimator=SGDClassifier(alpha=0.001, eta0=0.0012, fit_intercept=False,\n",
    "                                              l1_ratio=0.0, learning_rate=\"invscaling\", loss=\"hinge\", penalty=\"elasticnet\", power_t=0.5)),\n",
    "    BernoulliNB(alpha=2.300000000000001, fit_prior=False)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(cl12.steps, 'random_state', 422)\n",
    "\n",
    "etc = ExtraTreesClassifier(n_estimators=144,\n",
    "                           max_depth=589,\n",
    "                           criterion='entropy',\n",
    "                           class_weight='balanced_subsample',\n",
    "                           ccp_alpha=6.267622143679782e-05,\n",
    "                           min_samples_split=157,\n",
    "                           min_weight_fraction_leaf=4.8022857076483334e-05,\n",
    "                           min_impurity_decrease=1.5576259402879695e-05,\n",
    "                           max_features=0.00502175457189458,\n",
    "                           max_samples=0.8999810323985775,\n",
    "                           bootstrap=True)\n",
    "estimatorsLast = [(\"cl0\", cl0), (\"cl2\", cl2), (\"cl12\", cl12), (\"etc\", etc)]\n",
    "\n",
    "\n",
    "model = VotingClassifier(estimators=estimatorsLast,\n",
    "                         voting='soft',\n",
    "                         verbose=False,\n",
    "                         n_jobs=2, weights=weights)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b351c0",
   "metadata": {},
   "source": [
    "## How are most of our  models evaluated?\n",
    "\n",
    "### Cross Validation\n",
    "* To prevent potential overfitting issue, we use cross validation to evaluate the performance of our final model. We split the training dataset into **5 folds**, each time use **4** of them for training and **1** of them for validation. The labels of the dataset are also balanced (stratified) according to the ratio of positive and negative labels. After getting the result from all trials, we average the f1-score as our final evaluation matric. \n",
    "\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import cross_val_score\n",
    "## cross validation score\n",
    "cv2 = cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "score = cross_val_score(model, X, y, n_jobs=2, cv=cv2, scoring=\"f1_macro\")\n",
    "\n",
    "print(\"LATEST\")\n",
    "print(score)\n",
    "print(score.mean())\n",
    "print(score.std())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Analysis, Learning Points and Other considerations:\n",
    "\n",
    "### Increasing model complexity does not always increase model Performance\n",
    "* Simple models such as Naive Bayes can perform surprisingly well and sometimes even outperform complex models such as XGBoost etc. \n",
    "* An increase in model complexity increases the tendency of a model to overfitting, potentially leading to misleadingly high validation scores but poor test scores. Complex models also tend to take a longer time to train and test. Using them can be extremely resource intensive\n",
    "* It is hence necessary that a good balanced between simplicity and complexity is found, and that cross-validation is performed to ensure robustness of the results. \n",
    "\n",
    "### Real-world datasets are large and requires substantial resources to utilise for ML\n",
    "* Models, especially complex ones, can take a very long time to train on real-world datasets.\n",
    "* This means that model size/performance is an important consideration in real-world scenarios, and that small models are highly preferable.\n",
    "* Algorithms and preprocessors that perform dimensionality reduction/encoding of the dataset can increase model performance while decreasing the amount of time needed to train models. An in-depth practical and technical understanding of such preprocessors is hence extremely valuable.\n",
    "\n",
    "\n",
    "### PCA\n",
    "* In our cases, it appears that many models' performance decreased after doing PCA on data. In this particular case, it might be because: the features were already filtered for one time (we only choose the first 5000 words after tf-idf processing), and most noises have been filtered. It also indicates one of the disadvantages of PCA which is the loss of information. However, it is also true that the training time was much shorter after decreasing the number of features. We need to be careful in PCA operations and most of the time the balance of time, performance and the property of the dataset are keys that decide how we utilize PCA. In this project, it is clear that this dataset does not contain too much noise and we aim to maximize the performance, therefore, we did not apply PCA to the data before training.\n",
    "\n",
    "### Stacking Models\n",
    "* Even though almost all the models we used in task 3 have been taught in class, it is the first time we tried to stack them together. Compared to other ensemble techniques such bagging and boosting, stacking model is very different from them. Unlike bagging, in stacking, the models are typically different (e.g. not all decision trees) and fit on the same dataset (e.g. instead of samples of the training dataset). Unlike boosting, in stacking, a single model is used to learn how to best combine the predictions from the contributing models (e.g. instead of a sequence of models that correct the predictions of prior models). There are often 2 or more models responsible for fitting on the training data and predicting the result and one model for combine the predictions of these models. <br> Since it is easy to implement and quite effective, we could consider to include this technique in our syllabus.\n",
    "\n",
    "### Extra Tree Classifier: \n",
    "* Similar to random forest, extra tree classfier is another trees ensemble methods. While it is different from random forest since it adds more randomization and thus reduce bias and variance. In terms of computational cost, the extra trees algorithm is faster. Thus it provides another choice for us when we get stuck at increasing accuracy for random forest.\n",
    "\n",
    "### Techniques to fine tune the hyperparameters:\n",
    "* AutoML + Hyper-parameter tuning with TPOT\n",
    "* Optuna"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "80f22b69b9f8d68ace34ab34260f84f100690723be29e4bfe0f6c780c6b4105c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
